dbpdf.txt

Index Structures 

Chapter 4 



Outline 

• Indexes on sequential files 

• Secondary indexes 

• B-Trees 

• Hash Tables 



Introduction 

• SELECT * FROM R 
– Examine every block in the storage system 

– Enough information on block headers to identify where in the block 
records begin 

– Enough information in record headers to tell what relation the record 
belongs to 

• Better organization 
– Reserve some blocks, several cylinders for a given relation 

• Suppose we want to answer the following query 
– SELECT* FROM MOvieStar WHERE name=`Jim Carrey’ 

• Scan all the blocks on which MovieStar tuples could be found. 

• To execute the query quickly, we create one or more indexes. 



Indexes on Sequential Files 

• A sorted file is called the index file 

• A search key K in the index file is associated with a 

pointer to a data-file record that has search key K. 

• Dense index 

– There is an entry in the idex file for every record in the data 

file. 

• Sparse index 

– Only some of the data records are represented in the data 

file. 



5 

Sequential File 

20 
10 

40 
30 

60 
50 

80 
70 

100 
90 



6 

Sequential File 

20 
10 

40 
30 

60 
50 

80 
70 

100 
90 

Dense Index 

10 
20 
30 
40 

50 
60 
70 
80 

90 
100 
110 
120 



About Dense Index 
• The dense index supports queries that ask for 

records with a given search key value. 

• The index-based search is efficient 

– Number of index blocks is usually small compared 

with the number of data blocks 

– We can use binary search log2n of them 

– The index is small enough to be kept permanently 

in main memory buffers.  

• So requires only main memory accesses 

 



About Sparse Indexes 

• If a dense index is too large, we can use 

similar structure  called a sparse index 

– Uses less space but requires some more time to 

find a record. 

– A sparse index holds only one key-pointer to the 

first record on the block  



9 

Sequential File 

20 
10 

40 
30 

60 
50 

80 
70 

100 
90 

Sparse Index 

10 
30 
50 
70 

90 
110 
130 
150 

170 
190 
210 
230 



Multiple-levels  of index 

• As index can cover several blocks, search is 

required 

• By putting index on index we can make first 

level index more efficient. 



11 

Sequential File 

20 
10 

40 
30 

60 
50 

80 
70 

100 
90 

Sparse 2nd level 

10 
30 
50 
70 

90 
110 
130 
150 

170 
190 
210 
230 

10 
90 
170 
250 

330 
410 
490 
570 



Indexes with duplicate search keys 

• So far, search key is the key of the relation. 

• Indexes can be used on non-key attributes. 

– More than one record has the same key value 

• Have a dense index with one entry with key K 

for each record of the data file that has search 

key K. 



13 

Duplicate keys 

10 
10 

20 
10 

30 
20 

30 
30 

45 
40 



14 

10 
10 

20 
10 

30 
20 

30 
30 

45 
40 

10 
10 
10 
20 

20 
30 
30 
30 

10 
10 

20 
10 

30 
20 

30 
30 

45 
40 

10 
10 
10 
20 

20 
30 
30 
30 

Dense index, one way to implement? 

Duplicate keys 



15 

10 
10 

20 
10 

30 
20 

30 
30 

45 
40 

10 
20 
30 
40 

Dense index, better way? 

Duplicate keys 



16 

10 
10 

20 
10 

30 
20 

30 
30 

45 
40 

10 
10 
20 
30 

Sparse index, one way? 

Duplicate keys * First search key  on the each block of data file 

Finding the records 

•We first find the last entry E1 less than or equal to K 

• Move to the front  

•Either find the first 

•Find E2 with a key strictly less than K 



Managing Indexes During Data 

Modifications 

• Records will be inserted, deleted and sometimes 

updated. 

– As a result the sequential file will evolve. 

• Solution: 

– Create overflow blocks 

• Do not have entries in sparse index 

– Insert new blocks in the sequential order 

• New block needs an entry in the sparse index 

– If there is no space, slide tuples to  adjacent blocks. 

• There will be changes into index 



Index modifications 
• Creating or deleting overflow block  

– has no effect on dense index or sparse index as index is created on primary 
blocks 

• Creating or destroying blocks of the sequential file  
– has no effect on  dense index because the index refers to records not 

blocks. It does effect a sparse index, since we must insert or delete an 
index entry for the block created or destroyed. 

• Inserting or deleting records  
– has the same action on a dense index as a key-pointer pair for that record 

is inserted or deleted. But there is no effect of sparse index; if it is the first 
record of the block then the corresponding key value in the sparse index 
must be updated. 

• Sliding a record 
– Within a block or among the blocks, results in updating to the 

corresponding entry of dense index. 

– Sparse index is updated if the moved record is the first record.  



Index Modifications 

• When changes occur to data file, we must often change the index to 
adapt. 

• Strategy 
– An index file is an example of a sequential file; the key-pointer pairs can 

be treated as records sorted by the value of search key. 
• Same strategies adapted for data files can be used 

Action Dense Index Sparse Index 

Create empty overflow block none None 

Delete empty overflow block None None 

Create empty sequential block None Insert 

Delete empty sequential block None Delete 

Insert record Insert Update (?) 

Delete record Delete Update (?) 

Slide record update Update (?) 



20 

Deletion from dense index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
20 
30 
40 

50  

 
60 
70 
80 



21 

Deletion from dense index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
20 
30 
40 

50  

 
60 
70 
80 

– delete record 30 



22 

Deletion from dense index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
20 
30 
40 

50  

 
60 
70 
80 

– delete record 30 

40 



23 

Deletion from dense index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
20 
30 
40 

50  

 
60 
70 
80 

– delete record 30 

40 40 



24 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 



25 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete record 40 



26 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete record 40 



27 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete record 30 



28 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete record 30 

40 
40 



29 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete records 30 & 40 



30 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete records 30 & 40 



31 

Deletion from sparse index 

 

 

 
20 
10 

40 
30 

60 
50 

80 
70 

10 
30 
50 
70 

90  

 
110 
130 
150 

– delete records 30 & 40 

50 

70 



32 

Insertion, sparse index case 

 

 

 
20 
10 

30 

50 
40 

60 

10 
30 
40 
60 



33 

Insertion, sparse index case 

 

 

 
20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 34 



34 

Insertion, sparse index case 

 

 

 
20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 34 

34 

• our lucky day! 
   we have free space 
   where we need it! 



35 

Insertion, sparse index case 

 

 

 
20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 15 



36 

Insertion, sparse index case 

 

 

 
20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 15 

15 

20 
30 

20 



37 

Insertion, sparse index case 

 

 

 
20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 15 

15 

20 
30 

20 

• Illustrated: Immediate reorganization 
• Variation: 

– insert new block (chained file) 
– update index 



38 

Insertion, sparse index case 

 

 

 20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 25 



39 

Insertion, sparse index case 

 

 

 20 
10 

30 

50 
40 

60 

10 
30 
40 
60 

– insert record 25 

25 

overflow blocks 
(reorganize later...) 



40 

Insertion, dense index case 

• Similar 

• Often more expensive . . .  



Secondary Indexes 

• Primary indexes 

– Underlying file was sorted on the search key. 

• But we need a several indexes in the relation 

• In a MovieStar relation we declare name as a 

primary key 

• We may want to search on birth dates 

– We need a secondary index on birth date. 



Secondary Indexes 

• A secondary index facilitates finding records given a 

value for  one or more fields. 

• The secondary index does not determine the 

placement of records in a data file. 

• So, there is no sense to talk of sparse, secondary 

index  

– Since secondary index does not influence location, we 

could not use it to predict the location of any record whose 

key was not mentioned in the index file explicitely. 

– So secondary index is always dense. 



43 

Secondary indexes 
Sequence 
field 

50 
30 

70 
20 

40 
80 

10 
100 

60 
90 

• Sparse index 

30 
20 
80 
100 

90 
... 



44 

Secondary indexes 
Sequence 
field 

50 
30 

70 
20 

40 
80 

10 
100 

60 
90 

• Sparse index 

30 
20 
80 
100 

90 
... 

does not make sense! 



45 

Secondary indexes 
Sequence 
field 

50 
30 

70 
20 

40 
80 

10 
100 

60 
90 

• Dense index 



46 

Secondary indexes 
Sequence 
field 

50 
30 

70 
20 

40 
80 

10 
100 

60 
90 

• Dense index 

10 
20 
30 
40 

50 
60 
70 
... 



Design of Secondary Indexes 

• The keys in the index file are sorted. 

• The points in one index block can go to many 

different data blocks. 

– Using secondary index may result in many more 

disk I/Os. 

• It is possible to add a second level of index 



48 

Secondary indexes 
Sequence 
field 

50 
30 

70 
20 

40 
80 

10 
100 

60 
90 

• Dense index 

10 
20 
30 
40 

50 
60 
70 
... 

10 
50 
90 
... 

sparse 
high 
level 



Clustered File Structure 

• Two or more relations are stored with their records intermixed. 

• Consider the following two relations 
– Movie(title, year, length, studioName) 

– Studio(name, address, president) 

• Consider the following query 
– SELECT title, year 

– FROM Movie 

– WHERE studioName=‘zzz’; 

• If the above is typical query we can order the tuples with 
studioName 

• We can put a primary index on studioName 



Clustered File Structure 
• Consider the following query 

– SELECT president 

– FROM Movie, Studio 

– WHERE title=‘Star Wars’ AND Movie.studioName=Studio.name; 

• If we are sure that joins are common, we can make those joints 
efficient by chossing a clustered file structure. 

– Movie tuples are placed with Studio tuples in the same sequence of 
blocks. 

• We place for each studio tuple, all the movie tuples for the movies made 
by that studio. 

 

Studio 

1 

Movies by  

studio1 

Studio 

1 

Movies by  

studio1 



Indirection in Secondary Indexes 

• Significant amount of space is wasted  

– If the search key value appears n times in the data 

file, the value is written n times  in the index file. 

• Solution  

– Use the indirection  called buckets between 

secondary index file and the data file 



52 

Duplicate values & secondary indexes 

10 
20 

40 
20 

40 
10 

40 
10 

40 
30 

10 
20 
30 
40 

50 
60 
... 

buckets 



Advantages of bucket idea 

• We can use the pointers in the buckets to 

answer the queries 



54 

Why “bucket” idea is useful 

Indexes   Records 

Name: primary  EMP (name,dept,floor,...) 

Dept: secondary 

Floor: secondary 

 



55 

Query: Get employees in  

  (Toy Dept) ^ (2nd floor) 

Dept. index   EMP    Floor index 

Toy     2nd 



56 

Query: Get employees in  

  (Toy Dept) ^ (2nd floor) 

Dept. index   EMP    Floor index 

Toy     2nd 

? Intersect toy bucket and 2nd Floor         
    bucket to get set of matching EMP’s 



57 

IR QUERIES 

• Find articles with “cat” and “dog” 

• Find articles with “cat” or “dog” 

• Find articles with “cat” and not “dog” 



58 

IR QUERIES 

• Find articles with “cat” and “dog” 

• Find articles with “cat” or “dog” 

• Find articles with “cat” and not “dog” 

• Find articles with “cat” in title 

• Find articles with “cat” and “dog”  
  within 5 words 



59 

Common technique:  

      more info in inverted list 

cat 

 
Title 5 

Title 100 

Author 10 

Abstract 57 

Title 12 

d3 
d2 

d1 

dog 

 

Inverted Index for documents 



60 

Posting: an entry in inverted list. 

  Represents occurrence of 

  term in article 

Size of a list: 1 Rare words or 
  (in postings)    miss-spellings 

 

         106 Common words 

Size of a posting: 10-15 bits (compressed) 



61 

IR DISCUSSION 

• Stop words 

• Truncation 

• Thesaurus 

• Full text vs. Abstracts 

• Vector model 



62 

Vector space model 

       w1  w2  w3  w4  w5  w6  w7 … 

DOC =  <1     0    0    1     1    0    0  …> 

 

Query= <0     0    1    1     0    0    0 …> 



63 

Vector space model 

       w1  w2  w3  w4  w5  w6  w7 … 

DOC =  <1     0    0    1     1    0    0  …> 

 

Query= <0     0    1    1     0    0    0 …> 

PRODUCT =                  1 + ……. = score 



64 

• Tricks to weigh scores + normalize 

 

e.g.: Match on common word not as 

 useful as match on rare words... 



65 

• How to process V.S. Queries? 

 

     w1  w2  w3  w4  w5  w6    … 

Q = < 0     0    0    1     1    0    … > 



66 

Conventional indexes 

Advantage: 

   - Simple 
   - Index is sequential file 
     good for scans 

Disadvantage: 
   - Inserts expensive, and/or 
   - Lose sequentiality & balance 



67 

Example  Index (sequential) 
 

 

 

  continuous 

 

 

  free space 

10 
20 
30 

40 
50 
60 

70 
80 
90 



68 

Example  Index (sequential) 
 

 

 

  continuous 

 

 

  free space 

10 
20 
30 

40 
50 
60 

70 
80 
90 

39 
31 
35 
36 

32 
38 
34 

33 

overflow area 
(not sequential) 



B-trees 

• It is a general multi-level structure 

• B-tree  is a family of data structures 

– B+-tree is often used 

• B-trees automatically maintain as many levels 

of index as in appropriate for a file. 

• Every block is used between half and 

completely full. 

– No overflow blocks are needed 

 



Structure of Btree 

• The tree is balanced 

– All paths from root to a leaf have the same  length 

• Three layers 

– The root 

– Intermediate layer 

– and leaves 

• The parameter “n” is associated with each B-

tree index 

– Each block has a layout for “n” keys and “n+1” 

pointers 



Rules of Btree 
• At the root there are at least two pointers 

– All pointers point to B-tree blocks at the level below 

• At the leaf, one last pointer points to the next leaf block to the 
right (to the block with next higher keys) 
– At least ? n+1/2 ? pointers are used and point to data records. 

– Unused pointers may be thought of null and do not point any where. 

• At an interior node, all n+1 pointers can be used to point to B-
tree blocks at the next lower level. 
– At least ? n+1/2?  of them are actually used (for the root only two 

pointers are used) 

– If j pointers are used then there will be j-1 keys. 
• The first pointer points to part of Btree where some of the records less than 

k1 will be found.  

• The second pointer goes to that part of the tree where all records with keys  
that are at least K1, but less than K2 will be found. 

• The jth pointer  points to the part of BTree where some of the records 
greater than Kj-1 are found. 



72 

Sample non-leaf 

 

 

 

 
to keys to keys  to keys   to keys 

< 57  57? k<81  81?k<95  ?95 

 

5
7
 

 
8
1
 

 
9
5
 



73 

Root 

 

B+Tree Example    n=3 

1
0
0
 

1
2
0
 

1
5
0
 

1
8
0
 

3
0
 

3
 

5
 

1
1
 

3
0
 

3
5
 

1
0
0
 

1
0
1
 

1
1
0
 

1
2
0
 

1
3
0
 

1
5
0
 

1
5
6
 

1
7
9
 

1
8
0
 

2
0
0
 



74 

Sample leaf node: 

     From non-leaf node 

 

       to next leaf 

       in sequence 5
7
 

 
8
1
 

 
9
5
 

To
 r

e
co

rd
  

w
it
h
 k

e
y
 5

7
 

 
To

 r
e
co

rd
  

w
it
h
 k

e
y
 8

1
 

 
To

 r
e
co

rd
  

w
it
h
 k

e
y
 8

5
 



75 

In textbook’s notation   n=3 

Leaf: 

 

 

Non-leaf: 

 

3
0
 

3
5
 

3
0
 

30 35 

30 



76 

Size of nodes:  n+1 pointers 

     n keys   (fixed) 



77 

Don’t want nodes to be too empty 

• Use at least 

 

Non-leaf: ?(n+1)/2? pointers 

 

Leaf:  ?(n+1)/2?  pointers to data 



78 

    Full node  min. node 

 

Non-leaf 

 

 

Leaf 

n=3 

1
2
0
 

1
5
0
 

1
8
0
 

3
0
 

3
 

5
 

1
1
 

3
0
 

3
5
 

co
u
n
ts

 e
v
e
n
 i
f 

n
u
ll 



79 

B+tree rules  tree of order n 

(1) All leaves at same lowest level  

  (balanced tree) 

(2) Pointers in leaves point to records  

 except for “sequence pointer” 



80 

(3) Number of pointers/keys for B+tree 

 

Non-leaf 
(non-root) n+1 n ?(n+1)/2?  ?(n+1)/2?- 1 

Leaf 
(non-root) n+1 n 

Root n+1 n 1 1 

Max   Max  Min             Min  
ptrs   keys  ptrs?data    keys 

?(n+1)/2?  ?(n+1)/2? 



Applications of B-trees 

• B-tree is a powerful tool for building indexes 

• The search key is the primary key  for the data file 
and index is dense, there is a leaf pointer for every 
entry. The data file may not be sorted by primary key. 

• If the data file is sorted by primary key, the B+tree is 
a sparse index with key-pointer pair at the leaf for 
each block of the data file. 

• There is a variant of Btree for multiple occurrences of 
search key. 

– Refer text book.  



Lookup in Btree 

• Find a record with search-key value K 

• If we are at the leaf, look among the keys. If 

the ith leaf is K, ith pointer will take us to the 

desired record. 

• Follow the rules of Btree and reach the leaf 

node.  



Range queries 

• SELECT * 

• FROM R 

• WHERE R.k >40 

• Or 

• SELECT * 

• FROM R 

• WHERE R.k >= 10 and R.k <= 25; 

 



Insertion into Btree 

• We try to find a place 

• If there is no room in the leaf, we split the leaf 
into two and divide the keys into two nodes, so 
each is half full or just over half full. 

• The splitting of nodes lead to inserting a new 
key-pointer pair  at a higher level. 

– Apply this strategy recursively at higher levels. 

• We try to insert at the root. If there is no room 
we try to split the root and create a new root. 



Insert 
• Suppose N is a leaf node with n keys 

– We trying to insert “n+1” key and a pointer 

– Create a new node M, which will be a sibling of N, immediately to its 
right 

– The first (n+1)/2  (high) key-pointer pairs in the sorted order will be with 
N. The other key pointer pairs move to M 

– Both will have at least  (n+1)/2 (low)  key pointer pairs. 

• Suppose N is an interior node with n keys and n+1 pointers. 
– Create a new node M which will be a sibling of N, immediately to its 

right. 

– The first (n+2)/2  (high) pointers pairs in the sorted order will be with N 
and move M other (n+2)/2(low)  pointers. 

– The n/2 (high) keys stay with N, and last n/2  (low) keys move to M. 

– There is a one key left over, it is the smallest key reachable via M 
children. 

• K will be used as a parent of  N and M to divide the searches between those 
two nodes. 



86 

Insert into B+tree 

(a) simple case 
– space available in leaf 

(b) leaf overflow 

(c) non-leaf overflow 

(d) new root 

   



87 

(a) Insert key = 32 n=3 

3
 

5
 

1
1
 

3
0
 

3
1
 

 

3
0
 

1
0
0
 



88 

(a) Insert key = 32 n=3 

3
 

5
 

1
1
 

3
0
 

3
1
 

 

3
0
 

1
0
0
 

3
2
 



89 

(a) Insert key = 7 n=3 

3
 

5
 

1
1
 

3
0
 

3
1
 

 
3
0
 

1
0
0
 



90 

(a) Insert key = 7 n=3 

3
 

5
 

1
1
 

3
0
 

3
1
 

 
3
0
 

1
0
0
 

3
 

5
 7

 



91 

(a) Insert key = 7 n=3 

3
 

5
 

1
1
 

3
0
 

3
1
 

 
3
0
 

1
0
0
 

3
 

5
 7

 

7
 



92 

(c) Insert key = 160 

 

n=3 

1
0
0
 

 

1
2
0
 

1
5
0
 

1
8
0
 

1
5
0
 

1
5
6
 

1
7
9
 

1
8
0
 

2
0
0
 



93 

(c) Insert key = 160 

 

n=3 

1
0
0
 

 

1
2
0
 

1
5
0
 

1
8
0
 

1
5
0
 

1
5
6
 

1
7
9
 

1
8
0
 

2
0
0
 

1
6
0
 

1
7
9
 



94 

(c) Insert key = 160 

 

n=3 

1
0
0
 

 

1
2
0
 

1
5
0
 

1
8
0
 

1
5
0
 

1
5
6
 

1
7
9
 

1
8
0
 

2
0
0
 

1
8
0
 

1
6
0
 

1
7
9
 



95 

(c) Insert key = 160 

 

n=3 

1
0
0
 

 

1
2
0
 

1
5
0
 

1
8
0
 

1
5
0
 

1
5
6
 

1
7
9
 

1
8
0
 

2
0
0
 

1
6
0
 

1
8
0
 

1
6
0
 

1
7
9
 



96 

(d) New root,  insert 45 n=3 

1
0
 

2
0
 

3
0
 

1
 

2
 

3
 

1
0
 

1
2
 

2
0
 

2
5
 

3
0
 

3
2
 

4
0
 



97 

(d) New root,  insert 45 n=3 

1
0
 

2
0
 

3
0
 

1
 

2
 

3
 

1
0
 

1
2
 

2
0
 

2
5
 

3
0
 

3
2
 

4
0
 

4
0
 

4
5
 



98 

(d) New root,  insert 45 n=3 

1
0
 

2
0
 

3
0
 

1
 

2
 

3
 

1
0
 

1
2
 

2
0
 

2
5
 

3
0
 

3
2
 

4
0
 

4
0
 

4
5
 

4
0
 



99 

(d) New root,  insert 45 n=3 

1
0
 

2
0
 

3
0
 

1
 

2
 

3
 

1
0
 

1
2
 

2
0
 

2
5
 

3
0
 

3
2
 

4
0
 

4
0
 

4
5
 

4
0
 

3
0
 

new root 



100 

(a) Simple case - no example 

(b) Coalesce with neighbor (sibling) 

(c) Re-distribute keys 

(d) Cases (b) or (c) at non-leaf 

Deletion from B+tree 



Delete from BTree 

• Locate that record (lookup) 

• If after the deletion, the tree has minimum number of 
records, nothing to be done. 

• Move the key from one of the sibling and adjust the 
parents. 

• Hard case: no sibling has extra key 
– Merge the nodes by deleting one of them. 

– Adjust the keys at the parent. 

– If the parent is not full, we recursively continue the deletion 
process. 



102 

(b) Coalesce with sibling 

– Delete 50 

1
0
 

4
0
 

1
0
0
 

1
0
 

2
0
 

3
0
 

 

4
0
 

5
0
 

n=4 



103 

(b) Coalesce with sibling 

– Delete 50 

1
0
 

4
0
 

1
0
0
 

1
0
 

2
0
 

3
0
 

 

4
0
 

5
0
 

n=4 

4
0
 



104 

(c) Redistribute keys 

– Delete 50 

1
0
 

4
0
 

1
0
0
 

1
0
 

2
0
 

3
0
 

3
5
 

 
4
0
 

5
0
 

n=4 



105 

(c) Redistribute keys 

– Delete 50 

1
0
 

4
0
 

1
0
0
 

1
0
 

2
0
 

3
0
 

3
5
 

 
4
0
 

5
0
 

n=4 

3
5
 

3
5
 



106 

4
0
 

4
5
 

3
0
 

3
7
 

2
5
 

2
6
 

 

2
0
 

2
2
 

1
0
 

1
4
 

1
 

3
 

1
0
 

2
0
 

  

3
0
 

4
0
 

(d) Non-leaf coalese 

– Delete 37 
n=4 

2
5
 



107 

4
0
 

4
5
 

3
0
 

3
7
 

2
5
 

2
6
 

 

2
0
 

2
2
 

1
0
 

1
4
 

1
 

3
 

1
0
 

2
0
 

  

3
0
 

4
0
 

(d) Non-leaf coalese 

– Delete 37 
n=4 

3
0
 

2
5
 



108 

4
0
 

4
5
 

3
0
 

3
7
 

2
5
 

2
6
 

 

2
0
 

2
2
 

1
0
 

1
4
 

1
 

3
 

1
0
 

2
0
 

  

3
0
 

4
0
 

(d) Non-leaf coalese 

– Delete 37 
n=4 

4
0
 

3
0
 

2
5
 



109 

4
0
 

4
5
 

3
0
 

3
7
 

2
5
 

2
6
 

 

2
0
 

2
2
 

1
0
 

1
4
 

1
 

3
 

1
0
 

2
0
 

  

3
0
 

4
0
 

(d) Non-leaf coalese 

– Delete 37 
n=4 

4
0
 

3
0
 

2
5
 

2
5
 

new root 



Efficiency of Btrees 

• Very few disk I/Os per file operation 

• Splitting and merging blocks occur rarely 

• Keep the root in the main memory 

 



B+ Trees in Practice 

• Typical order: 100.  Typical fill-factor: 67%. 

– average fanout = 133 

• Typical capacities: 

– Height 4: 1334 = 312,900,700 records 

– Height 3: 1333 =     2,352,637 records 

• Can often hold top levels in buffer pool: 

– Level 1 =           1 page  =     8 Kbytes 

– Level 2 =      133 pages =     1 Mbyte 

– Level 3 = 17,689 pages = 133 MBytes        



Hash Tables 
• A hash function takes a search key as an argument 

and computes an integer range 0 to B-1, where B is a 
number of buckets. 

– A bucket array, which is an array indexed from 0 to B-1, 
holds headers of B linked lists, one for each bucket of the 
array. 

– If the record has a search key K, we store the record by 
linking it to the bucket list for the bucket numbered h(K), 
where h is the hash function.  

• Common hash function 
– Remainder of K/B 



Secondary Storage Hash Tables 

• Bucket contains blocks 

• If  a bucket overflows, a chain of overflow 

blocks  are added. 



Insertion into a Hash Table 

• When a new record with search key K must be 

inserted, we compute h(K). 

• If the bucket number h(K) has the space, we 

insert  the record into the block for this bucket 

or into one of the chain of blocks. 

• If there is no space, we add extra block. 

 



Hash-table insertion 

• Go to the bucket number h(K) and search for 

records with that search key. Delete if we find 

any data. 

– Consolidate (optional) 

 



116 

 

 

key ? h(key) 

Hashing 

<key> 

. 

. 

. 

Buckets 
(typically 1 
disk block) 



117 

. 

. 

. 

Two alternatives 

records 

. 

. 

. 

(1) key ? h(key) 



118 

 

 

 (2) key ? h(key) 

Index 

record 
key 1 

Two alternatives 



119 

 

 

 (2) key ? h(key) 

Index 

record 
key 1 

Two alternatives 

• Alt (2) for “secondary” search key 



120 

Example hash function 

• Key = ‘x1 x2 … xn’   n byte character string 

• Have b buckets 

• h:  add x1 + x2 + ….. xn 

–    compute sum modulo b 



121 

? This may not be best function … 

? Read Knuth Vol. 3 if you really  
 need to select a good function. 



122 

? This may not be best function … 

? Read Knuth Vol. 3 if you really  
 need to select a good function. 

Good hash ? Expected number of 

  function:  keys/bucket is the 

     same for all buckets 



123 

Within a bucket: 

• Do we keep keys sorted? 

• Yes, if CPU time critical 

   & Inserts/Deletes not too frequent 



124 

Next: example to illustrate   

  inserts, overflows, deletes 

   

   h(K) 



125 

EXAMPLE  2 records/bucket 

INSERT: 

h(a) = 1 

h(b) = 2 

h(c) = 1 

h(d) = 0 

0 

1 

 

2 

 

3 



126 

EXAMPLE  2 records/bucket 

INSERT: 

h(a) = 1 

h(b) = 2 

h(c) = 1 

h(d) = 0 

0 

1 

 

2 

 

3 

d 

a 

c 

b 

h(e) = 1 



127 

EXAMPLE  2 records/bucket 

INSERT: 

h(a) = 1 

h(b) = 2 

h(c) = 1 

h(d) = 0 

0 

1 

 

2 

 

3 

d 

a 

c 

b 

h(e) = 1 

e 



128 

0 

1 

 

2 

 

3 

a 

b 

c 

e 

d 

EXAMPLE:  deletion 

Delete: 
e 
f 

f 

g 



129 

0 

1 

 

2 

 

3 

a 

b 

c 

e 

d 

EXAMPLE:  deletion 

Delete: 
e 
f 

f 

g 
maybe move 

“g” up 

c 



130 

0 

1 

 

2 

 

3 

a 

b 

c 

e 

d 

EXAMPLE:  deletion 

Delete: 
e 
f 

f 

g 
maybe move 

“g” up 

c 
d 



131 

Rule of thumb: 

• Try to keep space utilization 

 between 50% and 80% 

      Utilization =    # keys used 
               total # keys that fit 



132 

Rule of thumb: 

• Try to keep space utilization 

 between 50% and 80% 

      Utilization =    # keys used 
               total # keys that fit 

• If < 50%, wasting space 

• If > 80%, overflows significant  
  depends on how good hash 
  function is & on # keys/bucket 



Efficiency of hash table indexes 

• Typical lookup takes only one disk I/O 

• Insertion/deletion takes only two disk I/Os. 

– Better than sparse or dense indexes 

– Btree 

• If the file grows, there will be many blocks in 
the chain  for a typical bucket and taking one 
disk I/O per block 

– So it is better to keep number of blocks per bucket 
low. 



Dynamic hash tables 

• So far we have discussed static hash tables 

– The number of buckets never changes. 

– Overflow problems 

• There are several kinds of dynamic hash 

tables, where B is allowed to vary.   

– Extensible hashing 

– Linear hashing 



Extensible Hash Tables 

• There is a level of indirection introduced for the buckets 

• An array of pointers to blocks represents the buckets instead of  
the array  consisting of the data blocks themselves. 

• The array of pointers can grow. The length is always a power 
of two 
– The number of blocks doubles 

• There need not be data block for each bucket; certain buckets  
can share a block if the total number of records in those 
buckets can fit in the block 

• The hash function computes a sequence of k-bits for some 
large k,  
– However, the bucket numbers will at all times use some smaller 

number of bits, say “i” bits from the beginning of sequence. 

– The bucket array will have 2i entries when i  is the number of bits used. 

 



Insertion into Extensible Hash Tables 

• To insert a record with search key K, we compute 
h(K), take the first i  bit sequence and go to the entry 
of the bucket array indexed by these i bits. 

• Follow the pointer 
– If there is a room insert  

– If there is no room, determine the number of bits to 
determine used to determine membership in block B. 

• If j<i  nothing needs to be done to bucket array 
– Split block B into two 

– Distribute records in B  to the new blocks based on j+1 bit. 

– Put j+1 in each blocks nub to indicate the number of bits used to 
determine the membership 

– Adjust the pointers in the bucket array so entries that formerly pointed 
to B now point  either to B or the new block depending on the j+1 bit. 



Insertion into Extensible Hash Tables 

• If j=i 

– Increment i by 1. 

– Double the length of the bucket array, so it now 

has 2i+1 entries. 



138 

Extensible hashing: two ideas 

(a) Use i of b bits output by hash function 

      b 

   h(K) ?  

 

     use i ? grows over time…. 

00110101 



139 

(b) Use directory 

 

 h(K)[i ]       to bucket 
. 
. 
. 

. 

. 

. 



140 

Example: h(k) is 4 bits; 2 keys/bucket 

i = 1 

1 

1 

0001 

1001 

1100 

Insert 1010 



141 

Example: h(k) is 4 bits; 2 keys/bucket 

i = 1 

1 

1 

0001 

1001 

1100 

Insert 1010 
1 

1100 

1010 



142 

Example: h(k) is 4 bits; 2 keys/bucket 

i = 1 

1 

1 

0001 

1001 

1100 

Insert 1010 
1 

1100 

1010 

New directory 

2 

00 

01 

10 

11 

i = 

2 

2 



143 

1 

0001 

2 

1001 

1010 

2 

1100 

Insert: 

0111 

0000 

00 

01 

10 

11 

2 i = 

Example continued 



144 

1 

0001 

2 

1001 

1010 

2 

1100 

Insert: 

0111 

0000 

00 

01 

10 

11 

2 i = 

Example continued 

0111 

0000 

0111 

0001 



145 

1 

0001 

2 

1001 

1010 

2 

1100 

Insert: 

0111 

0000 

00 

01 

10 

11 

2 i = 

Example continued 

0111 

0000 

0111 

0001 

2 

2 



146 

00 

01 

10 

11 

2 i = 

2 1001 

1010 

2 1100 

2 0111 

2 0000 

0001 

Insert: 

1001 

Example continued 



147 

00 

01 

10 

11 

2 i = 

2 1001 

1010 

2 1100 

2 0111 

2 0000 

0001 

Insert: 

1001 

Example continued 

1001 

1001 

1010 



148 

00 

01 

10 

11 

2 i = 

2 1001 

1010 

2 1100 

2 0111 

2 0000 

0001 

Insert: 

1001 

Example continued 

1001 

1001 

1010 

000 

001 

010 

011 

100 

101 

110 

111 

3 i = 

3 

3 



149 

Extensible hashing:  deletion 

• No merging of blocks 

• Merge blocks  

     and cut directory if possible 

  (Reverse insert procedure) 



150 

Deletion example: 

• Run thru insert example in reverse! 



151 

Note: Still need overflow chains 

• Example: many records with duplicate keys 

1 

1101 

1100 

2 

2 

1100 

insert 1100 

1100 

if we split: 



152 

Solution: overflow chains 

1 

1101 

1100 

1 

1100 

insert 1100 add overflow block: 

1101 

1101 



153 

   Extensible hashing 

 Can handle growing files 

  - with less wasted space 

  - with no full reorganizations 

Summary 

+ 

 Indirection 

  (Not bad if directory in memory) 

 Directory doubles in size 

  (Now it fits, now it does not) 

- 

- 



Linear Hashing 

• Advantages of extensible hashing 
– we need to search only one data block 

– We have to examine the bucket array 
• But if the bucket is small to fit in main memory, there is no disk I/O. 

• Problems with extensible hash 
– Doubling requires substantial work when “i” is large. 

• Interrupts access to data file 

– If it does not fit in main memory, may push other data that may be 
needed. 

• As a result the system may perform many disk I/Os 

– If the number of  records per block is small, one block may be split 
multiple times. 

• There might be million  bucket entries and the small number of data 
entries. 

• LINEAR HASHING allows growth of number of blocks very 
slowly. 

 



Linear Hashing: Strategy 

• The number of buckets n is equal to a fraction of 
number average number of records per block. 

• Since blocks can not always split, overflow blocks 
are allowed 

• The number of bits used to number of entries of 
bucket array is log2n, n is the number of buckets.  The 
bits are always taken from lower order side. 

• Suppose with “i” bits of the hash function, record 
with K is intended for bucket a1, a2,…,ai (last i bits 
of k).  



156 

Linear hashing 

• Another dynamic hashing scheme 

Two ideas: 

(a) Use i  low order bits of hash 
01110101 

grows 

b 

i 



157 

Linear hashing 

• Another dynamic hashing scheme 

Two ideas: 

(a) Use i  low order bits of hash 
01110101 

grows 

b 

i 

(b) File grows linearly 



Example 

i=1 

n=2 

r=3 

0000 

1010 

1111 

0 

1 

n is number of blocks= 2 

Hash function h produces four bits 

`i’ is number of bits in hash function 

n is number of current buckets 

r is number of records in a table 

 

The ratio of r/n will be limited to number of records in a block 

 In this case we ensure r <=1.7*n. 

So average occupancy of a bucket can not exceed 85%.  



Insertion 

• Compute h(k), where k is the key of the record 

• Determine the correct number of bits at the end of bit sequence 
to use as the bucket number 
– We put the record either in that bucket or in the bucket with the leading 

bit changed from 1 to 0. 

– If there is no room in the bucket we create a overflow block. 

• Each time we insert 
– Compute  ratio r/n 

– If it exceeds predetermined ratio (1.7 in this case), add a new bucket. 

• If n exceeds 2i,  “i” is incremented by one. All existing blocks 
get “0”  in front of their bit sequences.  

If h(k)[i ] ? n, then 

      look at bucket h(k)[i ] 

   else, look at bucket h(k)[i ] - 2i -1
    

 

Rule 



Example (contd.) 
• Add 0101 

– It goes to “1” bucket. 

– However the ration r/n exceeds 1.7, so add another bucket. Increment “i”.  

 

 

i=2 

n=3 

r=4 

0000 

0101 

1111 

00 

01 

1010 10 



Example (contd.) 
• Add 0001 

– Last two bits are 01  

– However the 01 bucket is full, so add an over flow block.  

– Since ration of r/n is less than 1.7 it is OK  

 

 

i=2 

n=4 

r=5 

0000 

0001 

0101 

00 

01 

1010 10 

1111 



Example (contd.) 
• Add 0111 

– Last two bits are 11  

– The bucket does not yet exist. So redirect this record to 01, whose number 
differs having a 0 in the first bit. The new record fits in the overflow bucket.  

– Since ration of r/n exceeded 1.7, we have to create a new bucket.  

 

 

i=2 

n=4 

r=6 

0000 

0001 

0101 

00 

01 

1010 10 

0111 

1111 

11 Next time, we insert 

a new record, we exceed 

The ratio, so “i” should be  

Incremented.  



163 

? When do we expand file? 

• Keep track of:     # used slots    
          total # of slots 

= U 



164 

• If U > threshold then increase m 

   (and maybe i ) 

? When do we expand file? 

• Keep track of:     # used slots    
          total # of slots 

= U 



165 

 Linear Hashing 

  Can handle growing files 

  - with less wasted space 

  - with no full reorganizations 

  

  No indirection like extensible hashing 

 

Summary 

+ 

+ 

  Can still have overflow chains - 



166 

Example: BAD CASE 

    Very full 
 

 

  Very empty    Need to move 

       m here… 

       Would waste 

       space... 

   



